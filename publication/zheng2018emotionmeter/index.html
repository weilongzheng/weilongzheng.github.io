<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.3.0">
  <meta name="generator" content="Hugo 0.54.0" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Wei-Long Zheng">

  
  
  
    
  
  <meta name="description" content="In this paper, we present a multimodal emotion recognition framework called EmotionMeter that combines brain waves and eye movements. To increase the feasibility and wearability of EmotionMeter in real-world applications, we design a six-electrode placement above the ears to collect electroencephalography (EEG) signals. We combine EEG and eye movements for integrating the internal cognitive states and external subconscious behaviors of users to improve the recognition accuracy of EmotionMeter . The experimental results demonstrate that modality fusion with multimodal deep neural networks can significantly enhance the performance compared with a single modality, and the best mean accuracy of 85.11% is achieved for four emotions (happy, sad, fear, and neutral). We explore the complementary characteristics of EEG and eye movements for their representational capacities and identify that EEG has the advantage of classifying happy emotion, whereas eye movements outperform EEG in recognizing fear emotion. To investigate the stability of EmotionMeter over time, each subject performs the experiments three times on different days. EmotionMeter obtains a mean recognition accuracy of 72.39% across sessions with the six-electrode EEG and eye movement features. These experimental results demonstrate the effectiveness of EmotionMeter within and between sessions.">

  
  <link rel="alternate" hreflang="en-us" href="https://weilongzheng.github.io/publication/zheng2018emotionmeter/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-134634212-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://weilongzheng.github.io/index.xml" type="application/rss+xml" title="Wei-Long Zheng">
  <link rel="feed" href="https://weilongzheng.github.io/index.xml" type="application/rss+xml" title="Wei-Long Zheng">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://weilongzheng.github.io/publication/zheng2018emotionmeter/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Wei-Long Zheng">
  <meta property="og:url" content="https://weilongzheng.github.io/publication/zheng2018emotionmeter/">
  <meta property="og:title" content="Emotionmeter: A multimodal framework for recognizing human emotions | Wei-Long Zheng">
  <meta property="og:description" content="In this paper, we present a multimodal emotion recognition framework called EmotionMeter that combines brain waves and eye movements. To increase the feasibility and wearability of EmotionMeter in real-world applications, we design a six-electrode placement above the ears to collect electroencephalography (EEG) signals. We combine EEG and eye movements for integrating the internal cognitive states and external subconscious behaviors of users to improve the recognition accuracy of EmotionMeter . The experimental results demonstrate that modality fusion with multimodal deep neural networks can significantly enhance the performance compared with a single modality, and the best mean accuracy of 85.11% is achieved for four emotions (happy, sad, fear, and neutral). We explore the complementary characteristics of EEG and eye movements for their representational capacities and identify that EEG has the advantage of classifying happy emotion, whereas eye movements outperform EEG in recognizing fear emotion. To investigate the stability of EmotionMeter over time, each subject performs the experiments three times on different days. EmotionMeter obtains a mean recognition accuracy of 72.39% across sessions with the six-electrode EEG and eye movement features. These experimental results demonstrate the effectiveness of EmotionMeter within and between sessions."><meta property="og:image" content="https://weilongzheng.github.io/publication/zheng2018emotionmeter/featured.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-02-08T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2018-02-08T00:00:00-05:00">
  

  

  

  <title>Emotionmeter: A multimodal framework for recognizing human emotions | Wei-Long Zheng</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Wei-Long Zheng</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/news/">
            
            <span>News</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/publication/">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#datasets">
            
            <span>Datasets</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#services">
            
            <span>Services</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/files/Resume_Weilong&#43;Zheng.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/publication/zheng2018emotionmeter/featured_hu5559dfe6f78f52c185c2eb39b4520195_804586_800x0_resize_box_2.png');"></div>
  <span class="article-header-caption">The fusion framework of EEG and eye movements using deep neural networks</span>
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Emotionmeter: A multimodal framework for recognizing human emotions</h1>

        

        



<meta content="2018-02-08 00:00:00 -0500 EST" itemprop="datePublished">
<meta content="2018-02-08 00:00:00 -0500 EST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/wei-long-zheng/"><strong>Wei-Long Zheng</strong></a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/wei-liu/">Wei Liu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/yifei-lu/">Yifei Lu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/bao-liang-lu/">Bao-Liang Lu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/andrzej-cichocki/">Andrzej Cichocki</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>February 2018</time>
  </span>
  

  

  

  
  

  

  

</div>


        







  






  



<div class="btn-links mb-3">
  
  







  


<a class="btn btn-outline-primary my-1 mr-1" href="/publication/zheng2018emotionmeter/zheng2018emotionmeter.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/zheng2018emotionmeter/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="http://bcmi.sjtu.edu.cn/~seed/seed-iv.html" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/emotion/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1109/TCYB.2018.2797176" target="_blank" rel="noopener">
  DOI
</a>



</div>



        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions&amp;url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f&amp;title=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f&amp;title=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions&amp;body=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/publication/zheng2018emotionmeter/featured_hu5559dfe6f78f52c185c2eb39b4520195_804586_680x500_fill_q90_box_smart1_2.png" itemprop="image" alt="">
        <span class="article-header-caption">The fusion framework of EEG and eye movements using deep neural networks</span>
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Emotionmeter: A multimodal framework for recognizing human emotions</h1>

  

  



<meta content="2018-02-08 00:00:00 -0500 EST" itemprop="datePublished">
<meta content="2018-02-08 00:00:00 -0500 EST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/wei-long-zheng/"><strong>Wei-Long Zheng</strong></a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/wei-liu/">Wei Liu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/yifei-lu/">Yifei Lu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/bao-liang-lu/">Bao-Liang Lu</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="/authors/andrzej-cichocki/">Andrzej Cichocki</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>February 2018</time>
  </span>
  

  

  

  
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions&amp;url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f&amp;title=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f&amp;title=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Emotionmeter%3a%20A%20multimodal%20framework%20for%20recognizing%20human%20emotions&amp;body=https%3a%2f%2fweilongzheng.github.io%2fpublication%2fzheng2018emotionmeter%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  







  






  



<div class="btn-links mb-3">
  
  







  


<a class="btn btn-outline-primary my-1 mr-1" href="/publication/zheng2018emotionmeter/zheng2018emotionmeter.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/zheng2018emotionmeter/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="http://bcmi.sjtu.edu.cn/~seed/seed-iv.html" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/emotion/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1109/TCYB.2018.2797176" target="_blank" rel="noopener">
  DOI
</a>



</div>


</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract" itemprop="text">In this paper, we present a multimodal emotion recognition framework called EmotionMeter that combines brain waves and eye movements. To increase the feasibility and wearability of EmotionMeter in real-world applications, we design a six-electrode placement above the ears to collect electroencephalography (EEG) signals. We combine EEG and eye movements for integrating the internal cognitive states and external subconscious behaviors of users to improve the recognition accuracy of EmotionMeter . The experimental results demonstrate that modality fusion with multimodal deep neural networks can significantly enhance the performance compared with a single modality, and the best mean accuracy of 85.11% is achieved for four emotions (happy, sad, fear, and neutral). We explore the complementary characteristics of EEG and eye movements for their representational capacities and identify that EEG has the advantage of classifying happy emotion, whereas eye movements outperform EEG in recognizing fear emotion. To investigate the stability of EmotionMeter over time, each subject performs the experiments three times on different days. EmotionMeter obtains a mean recognition accuracy of 72.39% across sessions with the six-electrode EEG and eye movement features. These experimental results demonstrate the effectiveness of EmotionMeter within and between sessions.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            

            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">IEEE Transactions on Cybernetics</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    




    






  
  
    
  
  








  </div>
</div>



<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.07fbebbbf71b021c8836e1d7ecffa489.js"></script>

  </body>
</html>

